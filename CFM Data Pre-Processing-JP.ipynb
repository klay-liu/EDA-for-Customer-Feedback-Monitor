{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFM Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import MeCab\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import fasttext\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_list = ['cfm_train_ja', 'cfm_test_ja', 'cfm_val_ja']\n",
    "\n",
    "for fn in filename_list:\n",
    "    if fn.find('train') > 0:\n",
    "        train_df = pd.read_csv(f'{fn}.csv', encoding = 'utf-8')\n",
    "    elif fn.find('test') > 0:\n",
    "        test_df = pd.read_csv(f'{fn}.csv', encoding = 'utf-8')\n",
    "    elif fn.find('val') > 0:\n",
    "        val_df = pd.read_csv(f'{fn}.csv', encoding = 'utf-8')\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lange_detect(series):\n",
    "    pretrained_model = \"lid.176.bin\" \n",
    "    model = fasttext.load_model(pretrained_model)\n",
    "    langs = []\n",
    "    for cm in series:\n",
    "        lang = model.predict(cm)[0]\n",
    "        langs.append(str(lang)[11:13])\n",
    "    return langs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the observations for raw dataset of Japanese comments is 76115\n",
      "The size of the observations for the cleaned dataset of Japanese comments is 50544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Japanese Comments df\n",
    "train_jp_df = train_df.loc[~train_df['comments_ja'].isna()].drop(columns=['comments_en', 'totalwords_en'])\n",
    "test_jp_df = test_df.loc[~test_df['comments_ja'].isna()].drop(columns=['comments_en'])\n",
    "test_jp_df['totalwords_ja'] = test_jp_df['comments_ja'].str.len()\n",
    "val_jp_df = val_df.loc[~val_df['comments_ja'].isna()].drop(columns=['comments_en'])\n",
    "val_jp_df['totalwords_ja'] = val_jp_df['comments_ja'].str.len()\n",
    "\n",
    "\n",
    "## Combined df for Japanese comments \n",
    "jp_dfs = [train_jp_df, test_jp_df, val_jp_df]\n",
    "## modify lang and rename comment col\n",
    "for j in jp_dfs:\n",
    "    j['lang'] = 'jp'\n",
    "    j.rename(columns = {'comments_ja': 'comments', 'totalwords_ja': 'init_totalwords'}, inplace =  True)\n",
    "all_jp_dfs = pd.concat(jp_dfs, sort = False)\n",
    "# remove punctuation\n",
    "all_jp_dfs['comments_no_punc'] = all_jp_dfs['comments'].str.replace('[^\\w\\s]','')\n",
    "all_jp_dfs['comments_no_punc']=all_jp_dfs['comments_no_punc'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "\n",
    "# detect language\n",
    "all_jp_dfs['lang_detect'] = lange_detect(all_jp_dfs['comments_no_punc'])\n",
    "print(f'The size of the observations for raw dataset of Japanese comments is {all_jp_dfs.shape[0]}')\n",
    "# Filter out the Japanese comments\n",
    "all_jp_dfs = all_jp_dfs[all_jp_dfs['lang_detect'].isin(['ja'])] \n",
    "print(f'The size of the observations for the cleaned dataset of Japanese comments is {all_jp_dfs.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>lang</th>\n",
       "      <th>label</th>\n",
       "      <th>init_totalwords</th>\n",
       "      <th>comments_no_punc</th>\n",
       "      <th>lang_detect</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>最悪の商品です。&lt;br /&gt;購入後、一週間たたずに発火しました。</td>\n",
       "      <td>jp</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>最悪の商品ですbr 購入後一週間たたずに発火しました</td>\n",
       "      <td>ja</td>\n",
       "      <td>最悪 商品 購入 一 週間 たた 発火 し</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>使用中に発火した。.</td>\n",
       "      <td>jp</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>使用中に発火した</td>\n",
       "      <td>ja</td>\n",
       "      <td>使用 発火 し</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USBコネクター部分が発火しました。.</td>\n",
       "      <td>jp</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>USBコネクター部分が発火しました</td>\n",
       "      <td>ja</td>\n",
       "      <td>USB コネクター 部分 発火 し</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ブレーカーを落として、電球とこちらを交換して、スイッチを入れたら爆発しましたけど、、、、、、、､めちゃくちゃ怖かったです。。。その後、冷えてから外して、元々点けていた電球を取り付けたら普通でした。こんな経験はじめてでした。.</td>\n",
       "      <td>jp</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>ブレーカーを落として電球とこちらを交換してスイッチを入れたら爆発しましたけどめちゃくちゃ怖かったですその後冷えてから外して元々点けていた電球を取り付けたら普通でしたこんな経験はじめてでした</td>\n",
       "      <td>ja</td>\n",
       "      <td>ブレーカー 落とし 電球 交換 し スイッチ 入れ 爆発 し めちゃくちゃ 怖かっ 後 冷え 外し 元々 点け い 電球 取り付け 普通 経験 はじめて</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>車載の充電器として使用していましたが、運転中に発火しました。たまたま止まっていたので問題はなかったですが、動いている時であれば事故につながっていました。&lt;br /&gt;返金していただきたいです。</td>\n",
       "      <td>jp</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>車載の充電器として使用していましたが運転中に発火しましたたまたま止まっていたので問題はなかったですが動いている時であれば事故につながっていましたbr 返金していただきたいです</td>\n",
       "      <td>ja</td>\n",
       "      <td>車載 充電 し 使用 し い 運転 発火 し たまたま 止まっ い 問題 なかっ 動い いる 時 あれ 事故 つながっ い br 返金 し いただき</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           comments  \\\n",
       "0                                                                                  最悪の商品です。<br />購入後、一週間たたずに発火しました。   \n",
       "1                                                                                                        使用中に発火した。.   \n",
       "2                                                                                               USBコネクター部分が発火しました。.   \n",
       "3  ブレーカーを落として、電球とこちらを交換して、スイッチを入れたら爆発しましたけど、、、、、、、､めちゃくちゃ怖かったです。。。その後、冷えてから外して、元々点けていた電球を取り付けたら普通でした。こんな経験はじめてでした。.   \n",
       "5                   車載の充電器として使用していましたが、運転中に発火しました。たまたま止まっていたので問題はなかったですが、動いている時であれば事故につながっていました。<br />返金していただきたいです。   \n",
       "\n",
       "  lang  label  init_totalwords  \\\n",
       "0   jp      1               32   \n",
       "1   jp      1               10   \n",
       "2   jp      1               19   \n",
       "3   jp      1              112   \n",
       "5   jp      1               95   \n",
       "\n",
       "                                                                                 comments_no_punc  \\\n",
       "0                                                                      最悪の商品ですbr 購入後一週間たたずに発火しました   \n",
       "1                                                                                        使用中に発火した   \n",
       "2                                                                               USBコネクター部分が発火しました   \n",
       "3  ブレーカーを落として電球とこちらを交換してスイッチを入れたら爆発しましたけどめちゃくちゃ怖かったですその後冷えてから外して元々点けていた電球を取り付けたら普通でしたこんな経験はじめてでした   \n",
       "5         車載の充電器として使用していましたが運転中に発火しましたたまたま止まっていたので問題はなかったですが動いている時であれば事故につながっていましたbr 返金していただきたいです   \n",
       "\n",
       "  lang_detect  \\\n",
       "0          ja   \n",
       "1          ja   \n",
       "2          ja   \n",
       "3          ja   \n",
       "5          ja   \n",
       "\n",
       "                                                                      tokenized  \n",
       "0                                                         最悪 商品 購入 一 週間 たた 発火 し  \n",
       "1                                                                       使用 発火 し  \n",
       "2                                                             USB コネクター 部分 発火 し  \n",
       "3  ブレーカー 落とし 電球 交換 し スイッチ 入れ 爆発 し めちゃくちゃ 怖かっ 後 冷え 外し 元々 点け い 電球 取り付け 普通 経験 はじめて  \n",
       "5    車載 充電 し 使用 し い 運転 発火 し たまたま 止まっ い 問題 なかっ 動い いる 時 あれ 事故 つながっ い br 返金 し いただき  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_jp = [ 'てる', 'いる', 'なる', 'れる', 'する', 'ある', 'こと', 'これ', 'さん', 'して', \\\n",
    "\n",
    "'くれる', 'やる', 'くださる', 'そう', 'せる', 'した', '思う', \\\n",
    "\n",
    "'それ', 'ここ', 'ちゃん', 'くん', '', 'て','に','を','は','の', 'が', 'と', 'た', 'し', 'で', \\\n",
    "\n",
    "'ない', 'も', 'な', 'い', 'か', 'ので', 'よう', '', 'れ','さ','なっ', 'br', '年', '月', '日']\n",
    "\n",
    "def mecab_parser(text):\n",
    "    mct = MeCab.Tagger()\n",
    "    mct.parse('')\n",
    "    node = mct.parseToNode(text)\n",
    "    output = []\n",
    "    \n",
    "    while node:\n",
    "        if node.surface != \"\":\n",
    "            word_type = node.feature.split(\",\")[0]\n",
    "            if word_type in [\"形容詞\", \"動詞\",\"名詞\", \"副詞\"]:\n",
    "                output.append(node.surface)\n",
    "        node = node.next\n",
    "        if node is None:\n",
    "            break\n",
    "    return \" \".join(output)\n",
    "\n",
    "all_jp_dfs['tokenized'] = all_jp_dfs['comments_no_punc'].apply(mecab_parser)\n",
    "all_jp_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jp_dfs = all_jp_dfs[['tokenized', 'label', 'init_totalwords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./clean_data/jp_comments_clean.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(all_jp_dfs, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
